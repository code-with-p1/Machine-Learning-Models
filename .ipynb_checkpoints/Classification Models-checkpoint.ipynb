{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e663fde3-e7ab-487d-957d-b3ac5e5af3bd",
   "metadata": {},
   "source": [
    "### Setup and Common Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a7f9a5f7-8759-4416-b99c-114ed2849a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all necessary libraries\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Import Datasets\n",
    "from sklearn.datasets import load_iris, load_wine, load_breast_cancer, make_classification\n",
    "\n",
    "# Function to quickly evaluate a model\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    predictions = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd75d568-18c3-4381-b572-2cb37b3d216f",
   "metadata": {},
   "source": [
    "### Model-Specific Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "db308c8c-0e50-4a45-a41e-8a7c32976671",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option A: Dataset 1 - load_iris() (Multi-class)\n",
    "data = load_iris()\n",
    "X, y = data.data, data.target\n",
    "dataset_name = \"Iris\"\n",
    "\n",
    "# Option B: Dataset 2 - load_wine() (Multi-class)\n",
    "data = load_wine()\n",
    "X, y = data.data, data.target\n",
    "dataset_name = \"Wine\"\n",
    "\n",
    "# Option C: Dataset 3 - load_breast_cancer() (Binary)\n",
    "data = load_breast_cancer()\n",
    "X, y = data.data, data.target\n",
    "dataset_name = \"Breast Cancer\"\n",
    "\n",
    "# Option D: Dataset 4 - make_classification() (Customizable)\n",
    "X, y = make_classification(n_samples=1000, n_features=10, n_classes=2, random_state=42)\n",
    "dataset_name = \"Synthetic Classification\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a546093f-ea34-4fa3-be83-9e6aefa4c8b4",
   "metadata": {},
   "source": [
    "# Classification Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a533bba6-40e3-4769-892e-ede7ddc65b8f",
   "metadata": {},
   "source": [
    "### 1. Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "952fc75f-7c56-42bd-83ba-229098280021",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression on Iris:\n",
      "Accuracy: 0.9667\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        10\n",
      "           1       1.00      0.90      0.95        10\n",
      "           2       0.91      1.00      0.95        10\n",
      "\n",
      "    accuracy                           0.97        30\n",
      "   macro avg       0.97      0.97      0.97        30\n",
      "weighted avg       0.97      0.97      0.97        30\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Load data (Choose one from Options A, B, C, or D above)\n",
    "data = load_iris()\n",
    "X, y = data.data, data.target\n",
    "dataset_name = \"Iris\"\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Create, train, and evaluate the model\n",
    "print(f\"Logistic Regression on {dataset_name}:\")\n",
    "model = LogisticRegression(max_iter=100, random_state=42) # max_iter increased for convergence\n",
    "model.fit(X_train, y_train)\n",
    "evaluate_model(model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec7b145-c41c-4b67-a622-3e1d6f92a6b1",
   "metadata": {},
   "source": [
    "### 2. Support Vector Classifier (SVC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "76cbaf66-46ce-460a-86fa-d1bd18efa44b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Support Vector Classifier (SVC) on Iris:\n",
      "Accuracy: 0.9667\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        10\n",
      "           1       1.00      0.90      0.95        10\n",
      "           2       0.91      1.00      0.95        10\n",
      "\n",
      "    accuracy                           0.97        30\n",
      "   macro avg       0.97      0.97      0.97        30\n",
      "weighted avg       0.97      0.97      0.97        30\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Note: SVMs often benefit from feature scaling.\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Create a pipeline that scales the data then applies SVC\n",
    "print(f\"Support Vector Classifier (SVC) on {dataset_name}:\")\n",
    "model = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('svc', SVC(kernel='rbf', random_state=42)) # 'rbf' kernel is common for non-linear problems\n",
    "])\n",
    "model.fit(X_train, y_train)\n",
    "evaluate_model(model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a74f50-fcaf-487d-a68a-53f4e7c3cf10",
   "metadata": {},
   "source": [
    "### 3. Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ad0e3ca7-bc05-4f97-9d60-8a240e1369e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Classifier on Iris:\n",
      "Accuracy: 0.9333\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        10\n",
      "           1       0.90      0.90      0.90        10\n",
      "           2       0.90      0.90      0.90        10\n",
      "\n",
      "    accuracy                           0.93        30\n",
      "   macro avg       0.93      0.93      0.93        30\n",
      "weighted avg       0.93      0.93      0.93        30\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Create, train, and evaluate the model\n",
    "print(f\"Decision Tree Classifier on {dataset_name}:\")\n",
    "model = DecisionTreeClassifier(max_depth=4, random_state=42) # Limiting depth prevents overfitting\n",
    "model.fit(X_train, y_train)\n",
    "evaluate_model(model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e83a6964-1fb7-444f-a9f7-6e91ab7e3edb",
   "metadata": {},
   "source": [
    "### 4. Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "99e6a627-142d-4d32-a9a4-303920beea08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Classifier on Iris:\n",
      "Accuracy: 0.9000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        10\n",
      "           1       0.82      0.90      0.86        10\n",
      "           2       0.89      0.80      0.84        10\n",
      "\n",
      "    accuracy                           0.90        30\n",
      "   macro avg       0.90      0.90      0.90        30\n",
      "weighted avg       0.90      0.90      0.90        30\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Create, train, and evaluate the model\n",
    "print(f\"Random Forest Classifier on {dataset_name}:\")\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42) # n_estimators = number of trees\n",
    "model.fit(X_train, y_train)\n",
    "evaluate_model(model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd9444e-20e4-4180-b33d-c4cc85b9b378",
   "metadata": {},
   "source": [
    "### 5. Gradient Boosting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "efc5b255-63fd-4788-a786-aa27877f50ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boosting Classifier on Iris:\n",
      "Accuracy: 0.9667\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        10\n",
      "           1       1.00      0.90      0.95        10\n",
      "           2       0.91      1.00      0.95        10\n",
      "\n",
      "    accuracy                           0.97        30\n",
      "   macro avg       0.97      0.97      0.97        30\n",
      "weighted avg       0.97      0.97      0.97        30\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Create, train, and evaluate the model\n",
    "print(f\"Gradient Boosting Classifier on {dataset_name}:\")\n",
    "model = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "evaluate_model(model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d9134a-e581-4b43-a60d-f096d98f0dd8",
   "metadata": {},
   "source": [
    "### 6. K-Nearest Neighbors Classifier (KNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c1575c6d-963f-4718-a065-2993277ce992",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K-Nearest Neighbors Classifier (KNN) on Iris:\n",
      "Accuracy: 0.9333\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        10\n",
      "           1       0.83      1.00      0.91        10\n",
      "           2       1.00      0.80      0.89        10\n",
      "\n",
      "    accuracy                           0.93        30\n",
      "   macro avg       0.94      0.93      0.93        30\n",
      "weighted avg       0.94      0.93      0.93        30\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Note: KNN also benefits greatly from feature scaling.\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Create a pipeline that scales the data then applies KNN\n",
    "print(f\"K-Nearest Neighbors Classifier (KNN) on {dataset_name}:\")\n",
    "model = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('knn', KNeighborsClassifier(n_neighbors=5)) # n_neighbors is the 'k' value\n",
    "])\n",
    "model.fit(X_train, y_train)\n",
    "evaluate_model(model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba6a5cf-a73d-43af-85c4-d0e1b3bcb5b8",
   "metadata": {},
   "source": [
    "### 7. Gaussian Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5d3cbbea-f4ae-4af4-8d53-7358cd4fa8db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gaussian Naive Bayes on Iris:\n",
      "Accuracy: 0.9667\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        10\n",
      "           1       1.00      0.90      0.95        10\n",
      "           2       0.91      1.00      0.95        10\n",
      "\n",
      "    accuracy                           0.97        30\n",
      "   macro avg       0.97      0.97      0.97        30\n",
      "weighted avg       0.97      0.97      0.97        30\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Create, train, and evaluate the model\n",
    "print(f\"Gaussian Naive Bayes on {dataset_name}:\")\n",
    "model = GaussianNB()\n",
    "model.fit(X_train, y_train)\n",
    "evaluate_model(model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a5cbdd-61bc-49a6-b90e-b7ae858dbd46",
   "metadata": {},
   "source": [
    "### 8. Linear Discriminant Analysis (LDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7025d12e-fd38-4855-adc0-b300b538a11a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Discriminant Analysis (LDA) on Iris:\n",
      "Accuracy: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        10\n",
      "           1       1.00      1.00      1.00        10\n",
      "           2       1.00      1.00      1.00        10\n",
      "\n",
      "    accuracy                           1.00        30\n",
      "   macro avg       1.00      1.00      1.00        30\n",
      "weighted avg       1.00      1.00      1.00        30\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Create, train, and evaluate the model\n",
    "print(f\"Linear Discriminant Analysis (LDA) on {dataset_name}:\")\n",
    "model = LinearDiscriminantAnalysis()\n",
    "model.fit(X_train, y_train)\n",
    "evaluate_model(model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3143fab8-185a-46a6-a09e-579058f0866d",
   "metadata": {},
   "source": [
    "## Complete Example Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8130c09e-c05e-4262-97b4-6a7a8452cf5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression\n",
      "Iris Dataset:\n",
      "Accuracy: 0.9667\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        10\n",
      "           1       1.00      0.90      0.95        10\n",
      "           2       0.91      1.00      0.95        10\n",
      "\n",
      "    accuracy                           0.97        30\n",
      "   macro avg       0.97      0.97      0.97        30\n",
      "weighted avg       0.97      0.97      0.97        30\n",
      "\n",
      "Wine Dataset:\n",
      "Accuracy: 0.9722\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        12\n",
      "           1       0.93      1.00      0.97        14\n",
      "           2       1.00      0.90      0.95        10\n",
      "\n",
      "    accuracy                           0.97        36\n",
      "   macro avg       0.98      0.97      0.97        36\n",
      "weighted avg       0.97      0.97      0.97        36\n",
      "\n",
      "Breast Cancer Dataset:\n",
      "Accuracy: 0.9649\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.93      0.95        42\n",
      "           1       0.96      0.99      0.97        72\n",
      "\n",
      "    accuracy                           0.96       114\n",
      "   macro avg       0.97      0.96      0.96       114\n",
      "weighted avg       0.97      0.96      0.96       114\n",
      "\n",
      "Synthetic Dataset:\n",
      "Accuracy: 0.8250\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.85      0.83       100\n",
      "           1       0.84      0.80      0.82       100\n",
      "\n",
      "    accuracy                           0.82       200\n",
      "   macro avg       0.83      0.82      0.82       200\n",
      "weighted avg       0.83      0.82      0.82       200\n",
      "\n",
      "***************************************************************************\n",
      "Support Vector Classifier (SVC)\n",
      "Iris Dataset:\n",
      "Accuracy: 0.9667\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        10\n",
      "           1       1.00      0.90      0.95        10\n",
      "           2       0.91      1.00      0.95        10\n",
      "\n",
      "    accuracy                           0.97        30\n",
      "   macro avg       0.97      0.97      0.97        30\n",
      "weighted avg       0.97      0.97      0.97        30\n",
      "\n",
      "Wine Dataset:\n",
      "Accuracy: 0.9722\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        12\n",
      "           1       0.93      1.00      0.97        14\n",
      "           2       1.00      0.90      0.95        10\n",
      "\n",
      "    accuracy                           0.97        36\n",
      "   macro avg       0.98      0.97      0.97        36\n",
      "weighted avg       0.97      0.97      0.97        36\n",
      "\n",
      "Breast Cancer Dataset:\n",
      "Accuracy: 0.9825\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.98      0.98        42\n",
      "           1       0.99      0.99      0.99        72\n",
      "\n",
      "    accuracy                           0.98       114\n",
      "   macro avg       0.98      0.98      0.98       114\n",
      "weighted avg       0.98      0.98      0.98       114\n",
      "\n",
      "Synthetic Dataset:\n",
      "Accuracy: 0.8450\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.89      0.85       100\n",
      "           1       0.88      0.80      0.84       100\n",
      "\n",
      "    accuracy                           0.84       200\n",
      "   macro avg       0.85      0.84      0.84       200\n",
      "weighted avg       0.85      0.84      0.84       200\n",
      "\n",
      "***************************************************************************\n",
      "Decision Tree Classifier\n",
      "Iris Dataset:\n",
      "Accuracy: 0.9333\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        10\n",
      "           1       0.90      0.90      0.90        10\n",
      "           2       0.90      0.90      0.90        10\n",
      "\n",
      "    accuracy                           0.93        30\n",
      "   macro avg       0.93      0.93      0.93        30\n",
      "weighted avg       0.93      0.93      0.93        30\n",
      "\n",
      "Wine Dataset:\n",
      "Accuracy: 0.9444\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.92      0.96        12\n",
      "           1       0.88      1.00      0.93        14\n",
      "           2       1.00      0.90      0.95        10\n",
      "\n",
      "    accuracy                           0.94        36\n",
      "   macro avg       0.96      0.94      0.95        36\n",
      "weighted avg       0.95      0.94      0.94        36\n",
      "\n",
      "Breast Cancer Dataset:\n",
      "Accuracy: 0.9386\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.93      0.92        42\n",
      "           1       0.96      0.94      0.95        72\n",
      "\n",
      "    accuracy                           0.94       114\n",
      "   macro avg       0.93      0.94      0.93       114\n",
      "weighted avg       0.94      0.94      0.94       114\n",
      "\n",
      "Synthetic Dataset:\n",
      "Accuracy: 0.9100\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.96      0.91       100\n",
      "           1       0.96      0.86      0.91       100\n",
      "\n",
      "    accuracy                           0.91       200\n",
      "   macro avg       0.91      0.91      0.91       200\n",
      "weighted avg       0.91      0.91      0.91       200\n",
      "\n",
      "***************************************************************************\n",
      "Random Forest Classifier\n",
      "Iris Dataset:\n",
      "Accuracy: 0.9000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        10\n",
      "           1       0.82      0.90      0.86        10\n",
      "           2       0.89      0.80      0.84        10\n",
      "\n",
      "    accuracy                           0.90        30\n",
      "   macro avg       0.90      0.90      0.90        30\n",
      "weighted avg       0.90      0.90      0.90        30\n",
      "\n",
      "Wine Dataset:\n",
      "Accuracy: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        12\n",
      "           1       1.00      1.00      1.00        14\n",
      "           2       1.00      1.00      1.00        10\n",
      "\n",
      "    accuracy                           1.00        36\n",
      "   macro avg       1.00      1.00      1.00        36\n",
      "weighted avg       1.00      1.00      1.00        36\n",
      "\n",
      "Breast Cancer Dataset:\n",
      "Accuracy: 0.9561\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.93      0.94        42\n",
      "           1       0.96      0.97      0.97        72\n",
      "\n",
      "    accuracy                           0.96       114\n",
      "   macro avg       0.96      0.95      0.95       114\n",
      "weighted avg       0.96      0.96      0.96       114\n",
      "\n",
      "Synthetic Dataset:\n",
      "Accuracy: 0.9000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.95      0.90       100\n",
      "           1       0.94      0.85      0.89       100\n",
      "\n",
      "    accuracy                           0.90       200\n",
      "   macro avg       0.90      0.90      0.90       200\n",
      "weighted avg       0.90      0.90      0.90       200\n",
      "\n",
      "***************************************************************************\n",
      "Gradient Boosting Classifier\n",
      "Iris Dataset:\n",
      "Accuracy: 0.9667\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        10\n",
      "           1       1.00      0.90      0.95        10\n",
      "           2       0.91      1.00      0.95        10\n",
      "\n",
      "    accuracy                           0.97        30\n",
      "   macro avg       0.97      0.97      0.97        30\n",
      "weighted avg       0.97      0.97      0.97        30\n",
      "\n",
      "Wine Dataset:\n",
      "Accuracy: 0.9444\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      1.00      0.96        12\n",
      "           1       0.93      0.93      0.93        14\n",
      "           2       1.00      0.90      0.95        10\n",
      "\n",
      "    accuracy                           0.94        36\n",
      "   macro avg       0.95      0.94      0.95        36\n",
      "weighted avg       0.95      0.94      0.94        36\n",
      "\n",
      "Breast Cancer Dataset:\n",
      "Accuracy: 0.9561\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.90      0.94        42\n",
      "           1       0.95      0.99      0.97        72\n",
      "\n",
      "    accuracy                           0.96       114\n",
      "   macro avg       0.96      0.95      0.95       114\n",
      "weighted avg       0.96      0.96      0.96       114\n",
      "\n",
      "Synthetic Dataset:\n",
      "Accuracy: 0.8950\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.93      0.90       100\n",
      "           1       0.92      0.86      0.89       100\n",
      "\n",
      "    accuracy                           0.90       200\n",
      "   macro avg       0.90      0.90      0.89       200\n",
      "weighted avg       0.90      0.90      0.89       200\n",
      "\n",
      "***************************************************************************\n",
      "K-Nearest Neighbors Classifier (KNN)\n",
      "Iris Dataset:\n",
      "Accuracy: 0.9333\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        10\n",
      "           1       0.83      1.00      0.91        10\n",
      "           2       1.00      0.80      0.89        10\n",
      "\n",
      "    accuracy                           0.93        30\n",
      "   macro avg       0.94      0.93      0.93        30\n",
      "weighted avg       0.94      0.93      0.93        30\n",
      "\n",
      "Wine Dataset:\n",
      "Accuracy: 0.9722\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        12\n",
      "           1       1.00      0.93      0.96        14\n",
      "           2       0.91      1.00      0.95        10\n",
      "\n",
      "    accuracy                           0.97        36\n",
      "   macro avg       0.97      0.98      0.97        36\n",
      "weighted avg       0.97      0.97      0.97        36\n",
      "\n",
      "Breast Cancer Dataset:\n",
      "Accuracy: 0.9561\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.93      0.94        42\n",
      "           1       0.96      0.97      0.97        72\n",
      "\n",
      "    accuracy                           0.96       114\n",
      "   macro avg       0.96      0.95      0.95       114\n",
      "weighted avg       0.96      0.96      0.96       114\n",
      "\n",
      "Synthetic Dataset:\n",
      "Accuracy: 0.8400\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.91      0.85       100\n",
      "           1       0.90      0.77      0.83       100\n",
      "\n",
      "    accuracy                           0.84       200\n",
      "   macro avg       0.85      0.84      0.84       200\n",
      "weighted avg       0.85      0.84      0.84       200\n",
      "\n",
      "***************************************************************************\n",
      "Gaussian Naive Bayes\n",
      "Iris Dataset:\n",
      "Accuracy: 0.9667\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        10\n",
      "           1       1.00      0.90      0.95        10\n",
      "           2       0.91      1.00      0.95        10\n",
      "\n",
      "    accuracy                           0.97        30\n",
      "   macro avg       0.97      0.97      0.97        30\n",
      "weighted avg       0.97      0.97      0.97        30\n",
      "\n",
      "Wine Dataset:\n",
      "Accuracy: 0.9722\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      1.00      0.96        12\n",
      "           1       1.00      0.93      0.96        14\n",
      "           2       1.00      1.00      1.00        10\n",
      "\n",
      "    accuracy                           0.97        36\n",
      "   macro avg       0.97      0.98      0.97        36\n",
      "weighted avg       0.97      0.97      0.97        36\n",
      "\n",
      "Breast Cancer Dataset:\n",
      "Accuracy: 0.9386\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.90      0.92        42\n",
      "           1       0.95      0.96      0.95        72\n",
      "\n",
      "    accuracy                           0.94       114\n",
      "   macro avg       0.94      0.93      0.93       114\n",
      "weighted avg       0.94      0.94      0.94       114\n",
      "\n",
      "Synthetic Dataset:\n",
      "Accuracy: 0.8550\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.93      0.87       100\n",
      "           1       0.92      0.78      0.84       100\n",
      "\n",
      "    accuracy                           0.85       200\n",
      "   macro avg       0.86      0.85      0.85       200\n",
      "weighted avg       0.86      0.85      0.85       200\n",
      "\n",
      "***************************************************************************\n",
      "Linear Discriminant Analysis (LDA)\n",
      "Iris Dataset:\n",
      "Accuracy: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        10\n",
      "           1       1.00      1.00      1.00        10\n",
      "           2       1.00      1.00      1.00        10\n",
      "\n",
      "    accuracy                           1.00        30\n",
      "   macro avg       1.00      1.00      1.00        30\n",
      "weighted avg       1.00      1.00      1.00        30\n",
      "\n",
      "Wine Dataset:\n",
      "Accuracy: 0.9444\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      1.00      0.96        12\n",
      "           1       0.93      0.93      0.93        14\n",
      "           2       1.00      0.90      0.95        10\n",
      "\n",
      "    accuracy                           0.94        36\n",
      "   macro avg       0.95      0.94      0.95        36\n",
      "weighted avg       0.95      0.94      0.94        36\n",
      "\n",
      "Breast Cancer Dataset:\n",
      "Accuracy: 0.9561\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.90      0.94        42\n",
      "           1       0.95      0.99      0.97        72\n",
      "\n",
      "    accuracy                           0.96       114\n",
      "   macro avg       0.96      0.95      0.95       114\n",
      "weighted avg       0.96      0.96      0.96       114\n",
      "\n",
      "Synthetic Dataset:\n",
      "Accuracy: 0.8450\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.89      0.85       100\n",
      "           1       0.88      0.80      0.84       100\n",
      "\n",
      "    accuracy                           0.84       200\n",
      "   macro avg       0.85      0.84      0.84       200\n",
      "weighted avg       0.85      0.84      0.84       200\n",
      "\n",
      "***************************************************************************\n"
     ]
    }
   ],
   "source": [
    "# --- COMPLETE EXAMPLE: All 4 datasets ---\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import load_iris, load_wine, load_breast_cancer, make_classification\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    predictions = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(classification_report(y_test, predictions))\n",
    "\n",
    "# Define the datasets\n",
    "datasets = {\n",
    "    \"Iris\": load_iris(),\n",
    "    \"Wine\": load_wine(),\n",
    "    \"Breast Cancer\": load_breast_cancer(),\n",
    "    \"Synthetic\": make_classification(n_samples=1000, n_features=10, n_classes=2, random_state=42)\n",
    "}\n",
    "\n",
    "all_models = {}\n",
    "\n",
    "# Initialize the models\n",
    "model = LogisticRegression(max_iter=100, random_state=42) # max_iter increased for convergence\n",
    "all_models.update({'Logistic Regression' : model})\n",
    "\n",
    "model = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('svc', SVC(kernel='rbf', random_state=42)) # 'rbf' kernel is common for non-linear problems\n",
    "])\n",
    "all_models.update({'Support Vector Classifier (SVC)' : model})\n",
    "\n",
    "model = DecisionTreeClassifier(max_depth=4, random_state=42) # Limiting depth prevents overfitting\n",
    "all_models.update({'Decision Tree Classifier' : model})\n",
    "\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42) # n_estimators = number of trees\n",
    "all_models.update({'Random Forest Classifier' : model})\n",
    "\n",
    "model = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, random_state=42)\n",
    "all_models.update({'Gradient Boosting Classifier' : model})\n",
    "\n",
    "model = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('knn', KNeighborsClassifier(n_neighbors=5)) # n_neighbors is the 'k' value\n",
    "])\n",
    "all_models.update({'K-Nearest Neighbors Classifier (KNN)' : model})\n",
    "\n",
    "model = GaussianNB()\n",
    "all_models.update({'Gaussian Naive Bayes' : model})\n",
    "\n",
    "model = LinearDiscriminantAnalysis()\n",
    "all_models.update({'Linear Discriminant Analysis (LDA)' : model})\n",
    "\n",
    "# Train and evaluate on each dataset\n",
    "for model_name, model in all_models.items(): \n",
    "    print(f\"{model_name}\")\n",
    "    for name, data in datasets.items():\n",
    "        if name == \"Synthetic\":\n",
    "            X, y = data\n",
    "        else:\n",
    "            X, y = data.data, data.target\n",
    "            \n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        print(f\"{name} Dataset:\")\n",
    "        evaluate_model(model, X_test, y_test)\n",
    "    print(\"*\"*75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c79892-f8c0-4eaf-804b-279934264e8a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
